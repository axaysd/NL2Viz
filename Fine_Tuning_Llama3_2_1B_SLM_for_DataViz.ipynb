{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btzGpI6qd9MK"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "!pip install datasets\n",
        "!pip install datasets transformers\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the updated dataset\n",
        "dataset_path = \"dataset.csv\"\n",
        "dataset = pd.read_csv(dataset_path)\n",
        "\n",
        "print(\"Dataset Loaded:\")\n",
        "print(dataset.head())"
      ],
      "metadata": {
        "id": "0-H4vgrDeXvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Convert the Pandas DataFrame to Hugging Face Dataset\n",
        "hf_dataset = Dataset.from_pandas(dataset)\n",
        "\n",
        "print(\"Dataset successfully converted to Hugging Face Dataset format:\")\n",
        "print(hf_dataset)"
      ],
      "metadata": {
        "id": "dJkWfeHMehPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Llama-3.2-1B-Instruct\")\n",
        "\n",
        "# Define the preprocessing function\n",
        "def formatting_prompts_func(examples):\n",
        "    inputs = examples[\"Input\"]\n",
        "    outputs = examples[\"Output\"]\n",
        "    return {\n",
        "        \"input_ids\": [tokenizer(input_text, truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"] for input_text in inputs],\n",
        "        \"labels\": [tokenizer(output_text, truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"] for output_text in outputs],\n",
        "    }\n",
        "\n",
        "# Apply preprocessing\n",
        "processed_dataset = hf_dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "print(\"Dataset successfully tokenized and preprocessed:\")\n",
        "print(processed_dataset)"
      ],
      "metadata": {
        "id": "y1Nij8rmejL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into training and validation datasets\n",
        "train_dataset, val_dataset = processed_dataset.train_test_split(test_size=0.1).values()\n",
        "\n",
        "print(f\"Training Set: {len(train_dataset)} examples\")\n",
        "print(f\"Validation Set: {len(val_dataset)} examples\")"
      ],
      "metadata": {
        "id": "H1uUUhADekV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Load the base model\n",
        "model_name = \"unsloth/Llama-3.2-1B-Instruct\"\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=512,\n",
        "    load_in_4bit=True,        # Enable 4-bit quantization for efficiency\n",
        "    dtype=None,               # Auto-detect dtype\n",
        "    token=\"hf_your_token_here\",  # Replace with your Hugging Face token if required\n",
        ")\n",
        "\n",
        "# Configure the model with LoRA\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,  # LoRA rank\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    use_gradient_checkpointing=True,  # Enable gradient checkpointing\n",
        ")\n",
        "\n",
        "print(\"Model configured with LoRA for fine-tuning.\")"
      ],
      "metadata": {
        "id": "9WtohIyteojF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",               # Directory for saving checkpoints\n",
        "    evaluation_strategy=\"epoch\",          # Evaluate after each epoch\n",
        "    learning_rate=2e-5,                   # Learning rate\n",
        "    per_device_train_batch_size=8,        # Training batch size\n",
        "    per_device_eval_batch_size=8,         # Evaluation batch size\n",
        "    num_train_epochs=3,                   # Number of epochs\n",
        "    weight_decay=0.01,                    # Weight decay for regularization\n",
        "    save_steps=500,                       # Save checkpoint every 500 steps\n",
        "    save_total_limit=2,                   # Keep only 2 checkpoints\n",
        "    logging_dir=\"./logs\",                 # Directory for logs\n",
        "    logging_steps=10,                     # Log every 10 steps\n",
        ")\n",
        "\n",
        "print(\"Training arguments configured.\")"
      ],
      "metadata": {
        "id": "9GY4jRqmeqUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "# Set up the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,                          # Model with LoRA\n",
        "    args=training_args,                   # Training arguments\n",
        "    train_dataset=train_dataset,          # Training dataset\n",
        "    eval_dataset=val_dataset,             # Validation dataset\n",
        "    tokenizer=tokenizer,                  # Tokenizer\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "print(\"Fine-tuning completed.\")"
      ],
      "metadata": {
        "id": "Sduv0etOfcgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the model for inference\n",
        "model = FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Define a function to generate Matplotlib code\n",
        "def generate_code(query):\n",
        "    inputs = tokenizer(query, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
        "    outputs = model.generate(**inputs, max_length=512)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Example queries\n",
        "test_queries = [\n",
        "    \"Show the distribution of expenses using Matplotlib. Columns: categories, expenses\",\n",
        "    \"Visualize sales trends over the last quarter with Matplotlib. Use columns months, sales\",\n",
        "    \"Compare revenues by region using Matplotlib. Columns: regions, revenues\",\n",
        "]\n",
        "\n",
        "# Generate and print Matplotlib code for each query\n",
        "for query in test_queries:\n",
        "    generated_code = generate_code(query)\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Generated Matplotlib Code:\\n{generated_code}\\n\")"
      ],
      "metadata": {
        "id": "w6TswPKdf3qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Evaluation"
      ],
      "metadata": {
        "id": "tTDSgEXDvVb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Pretrained model\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Load the original pretrained model\n",
        "pretrained_model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Llama-3.2-1B-Instruct\",\n",
        "    max_seq_length=512,\n",
        "    load_in_4bit=True,\n",
        "    dtype=None,\n",
        "    token=\"hf_your_token_here\"\n",
        ")"
      ],
      "metadata": {
        "id": "Drsm0Pn7vzO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defined fine_tuned_model with our fine-tuned model\n",
        "fine_tuned_model = model"
      ],
      "metadata": {
        "id": "xq4wzOYGv8MJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Prepare both models for inference\n",
        "pretrained_model = FastLanguageModel.for_inference(pretrained_model)\n",
        "fine_tuned_model = FastLanguageModel.for_inference(fine_tuned_model)\n",
        "\n",
        "# Example queries\n",
        "test_queries = [\n",
        "    \"Show the distribution of expenses using Matplotlib. Columns: categories, expenses\",\n",
        "    \"Visualize sales trends over the last quarter with Matplotlib. Use columns months, sales\",\n",
        "    \"Compare revenues by region using Matplotlib. Columns: regions, revenues\",\n",
        "]\n",
        "\n",
        "# Function to generate code\n",
        "def generate_code(model, tokenizer, query):\n",
        "    inputs = tokenizer(query, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
        "    outputs = model.generate(**inputs, max_length=512)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Evaluate both pretrained and fine-tuned models\n",
        "results = []\n",
        "for query in test_queries:\n",
        "    pretrained_code = generate_code(pretrained_model, tokenizer, query)  # Pretrained model's output\n",
        "    fine_tuned_code = generate_code(fine_tuned_model, tokenizer, query)  # Fine-tuned model's output\n",
        "    results.append({\n",
        "        \"Query\": query,\n",
        "        \"Pretrained Output\": pretrained_code,\n",
        "        \"Fine-Tuned Output\": fine_tuned_code\n",
        "    })\n",
        "\n",
        "# Print results\n",
        "for result in results:\n",
        "    print(f\"Query: {result['Query']}\")\n",
        "    print(f\"Pretrained Model Output:\\n{result['Pretrained Output']}\\n\")\n",
        "    print(f\"Fine-Tuned Model Output:\\n{result['Fine-Tuned Output']}\\n\")\n"
      ],
      "metadata": {
        "id": "GEX6p9gmvRRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import evaluate\n",
        "\n",
        "# Prepare both models for inference\n",
        "pretrained_model = FastLanguageModel.for_inference(pretrained_model)\n",
        "fine_tuned_model = FastLanguageModel.for_inference(fine_tuned_model)\n",
        "\n",
        "# Example queries and reference outputs from the dataset\n",
        "test_queries = [example[\"Input\"] for example in hf_dataset]\n",
        "reference_outputs = [example[\"Output\"] for example in hf_dataset]\n",
        "\n",
        "# Function to extract only the Python code from the output\n",
        "def extract_code(output):\n",
        "    start_marker = \"```python\"\n",
        "    end_marker = \"```\"\n",
        "    if start_marker in output and end_marker in output:\n",
        "        start_idx = output.index(start_marker) + len(start_marker)\n",
        "        end_idx = output.index(end_marker, start_idx)\n",
        "        return output[start_idx:end_idx].strip()\n",
        "    return output.strip()\n",
        "\n",
        "# Modified function to generate code and extract only the Python code\n",
        "def generate_code_with_extraction(model, tokenizer, query):\n",
        "    raw_output = generate_code(model, tokenizer, query)\n",
        "    return extract_code(raw_output)\n",
        "\n",
        "# BLEU Score Evaluation\n",
        "def calculate_bleu_score(fine_tuned_model, tokenizer, test_queries, reference_outputs):\n",
        "    # Generate predictions with code extraction\n",
        "    predictions = [generate_code_with_extraction(fine_tuned_model, tokenizer, query) for query in test_queries]\n",
        "\n",
        "    # Extract only the code from references for comparison\n",
        "    reference_codes = [extract_code(ref) for ref in reference_outputs]\n",
        "\n",
        "    # Load BLEU metric\n",
        "    bleu = evaluate.load(\"bleu\")\n",
        "    return bleu.compute(predictions=predictions, references=[[ref] for ref in reference_codes])[\"bleu\"]\n",
        "\n",
        "# Calculate BLEU score\n",
        "bleu_score = calculate_bleu_score(fine_tuned_model, tokenizer, test_queries, reference_outputs)\n",
        "print(f\"BLEU Score: {bleu_score}\")\n",
        "\n",
        "# Executability Testing\n",
        "def test_executability(code):\n",
        "    try:\n",
        "        exec(code, {\"__builtins__\": {}})\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        return False\n",
        "\n",
        "# Test executability for pretrained and fine-tuned models\n",
        "executability_results = {\n",
        "    \"Pretrained\": [],\n",
        "    \"Fine-Tuned\": []\n",
        "}\n",
        "\n",
        "for query in test_queries:\n",
        "    pretrained_code = generate_code_with_extraction(pretrained_model, tokenizer, query)\n",
        "    fine_tuned_code = generate_code_with_extraction(fine_tuned_model, tokenizer, query)\n",
        "    executability_results[\"Pretrained\"].append(test_executability(pretrained_code))\n",
        "    executability_results[\"Fine-Tuned\"].append(test_executability(fine_tuned_code))\n",
        "\n",
        "# Calculate executability rates\n",
        "exec_rate_pretrained = sum(executability_results[\"Pretrained\"]) / len(executability_results[\"Pretrained\"]) * 100\n",
        "exec_rate_fine_tuned = sum(executability_results[\"Fine-Tuned\"]) / len(executability_results[\"Fine-Tuned\"]) * 100\n",
        "\n",
        "print(f\"Pretrained Model Executability Rate: {exec_rate_pretrained:.2f}%\")\n",
        "print(f\"Fine-Tuned Model Executability Rate: {exec_rate_fine_tuned:.2f}%\")"
      ],
      "metadata": {
        "id": "hIButtBIwkgW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}